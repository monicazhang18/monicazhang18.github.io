---
title: "机器学习吃瓜 - 第1 & 2 章"
description: "西瓜书+南瓜书: 概览、 模型评估与选择"
author: yoyo
date: 2024-09-16 16:11:00 +0800
categories: [Artificial Intelligence, Datawhale, Pumpkin]
tags: [Machine Learning Theory]
math: true
image:
  path: /assets/covers/datawhale/pumpkin-book.jpeg
  alt: Datawhale Pumpkin Book Chapter 1
---

## 基本术语

### 机器学习术语与概念
![机器学习术语与概念](/assets/image/AI/Datawhale/pumpkin/chapter-1-2/ml-terms.jpeg)

### 机器学习的分类

根据训练数据是否带有标记，机器学习可分为 **有监督学习 (Supervised Learning)** 和 **无监督学习 (Unsupervised Learning)**。

### 1. 有监督学习 (Supervised Learning)

有监督学习是指使用带有标记的训练数据来构建模型。每个训练样本都包括输入和对应的期望输出，模型的任务是学习从输入到输出的映射关系。常见的有监督学习任务包括：

- **分类 (Classification)**: 将输入数据分配到预定义的离散类别中。
  
- **回归 (Regression)**: 预测连续的数值结果。

### 2. 无监督学习 (Unsupervised Learning)

无监督学习是指使用没有标记的训练数据进行模型训练。模型的任务是从数据中发现潜在的结构或模式，而不需要任何显式的标记。常见的无监督学习任务包括：

- **聚类 (Clustering)**: 将数据根据其内在特征划分为不同的组或簇。
  
- **降维 (Dimensionality Reduction)**: 在保持数据重要特征的前提下，减少数据的维度。
  
### 数据集的组成
![数据集的组成](/assets/image/AI/Datawhale/pumpkin/chapter-1-2/dataset.jpg)

**泛化能力 Generalization**：
  
  - 对新样本的预测能力

**分布 Probability distribution**：

  - 假设样本空间服从一个未知分布$D$， 机器学习的样本则是从该分布中采样得到，即“独立间分布” ([Independent and Identically distributed (IID)](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables))。训练的样本越多，越有可能通过学习获得具有强泛化能力的模型。

## 假设空间

### 机器学习 -> 归纳学习

- **归纳 (Induction)**  
  归纳是指从特殊到一般的“泛化”过程。模型通过观察有限的训练数据，推测出能够应用到新数据上的一般规则。
  
- **演绎 (Deduction)**  
  演绎是指从一般到特殊的“特化”过程。它基于已有的一般规则，推导出具体实例的结论。

### 假设空间 (Hypothesis Space)

假设空间是指机器学习算法在搜索过程中可能的所有假设集合。换句话说，假设空间定义了模型可以选取的所有可能的解决方案。学习的过程可以理解为在假设空间中搜索一个最优假设，来解释观察到的训练数据。

### 版本空间 (Version Space)

版本空间是指假设空间中的一部分，它包含所有与当前训练数据一致的假设。版本空间的概念帮助我们理解在假设空间中，哪些假设是可能有效的，哪些是假设可以被排除的。在模型训练的过程中，版本空间通常会不断缩小，最终收敛到一个或者几个最优假设。

## 归纳偏好

不同的机器学习算法有不同的归纳偏好：通过相同的数据集训练，得到不一样的预测模型。

### 奥卡姆剃刀 Occam's razor**

> [奥卡姆剃刀](https://en.wikipedia.org/wiki/Occam%27s_razor)：若有多个假设与观察一直，则选最简单的那个。

奥卡姆剃刀强调“选择最简单的假设”，它实际上是一种归纳偏好。不同的机器学习算法在构建模型时具有不同的偏好，而某些算法可能更倾向于遵循奥卡姆剃刀原理，即更偏好简单的模型。这种偏好有助于模型在面对新数据时避免过拟合，因为简单的模型通常泛化能力更强。

**没有免费的午餐定理 No free lunch**

> [没有免费的午餐定理](https://en.wikipedia.org/wiki/No_free_lunch_theorem)：没有一个算法在所有的问题中都是最优解。

这一定理指出，没有一个算法能够在所有问题中都是最优解。也就是说，对于不同类型的问题，可能需要不同的算法来达到最佳效果。这也是为什么在实际应用中，算法的选择需要根据问题的具体特点来决定。

## 经验误差与过拟合

### 误差

误差是指学习器的实际预测输出与样本真实输出之间的差异。根据数据集的不同，误差可以分为两类：

- **训练误差 (Training Error)** 或 **经验误差 (Empirical Error)**：在训练集上产生的误差，表示模型在已知样本上的表现。
- **泛化误差 (Generalization Error)**：在新样本（即未见过的数据）上产生的误差，表示模型在未知数据上的表现。

如果在一个包含 $ m $ 个样本的数据集中，有 $ a $ 个样本被错误分类，则错误率和精度可以定义为：

- **错误率 (Error Rate)**：  
  $$
  \text{错误率} = \frac{a}{m}
  $$

- **精度 (Accuracy)**：  
  $$
  \text{精度} = 1 - \frac{a}{m}
  $$

### 过拟合与欠拟合

#### 过拟合 (Overfitting)

过拟合是指模型对训练数据中的噪声或偶然性特征进行了过度拟合，从而在训练集上表现良好，但在新数据上表现较差。此时模型将训练数据的特殊性（包括噪声）当作所有潜在样本的普遍性质，导致泛化能力下降。过拟合倾向于过度拟合训练数据中的噪声，表现为低训练误差但高泛化误差。

#### 欠拟合 (Underfitting)

欠拟合是指模型对训练数据的基本特征学习得不充分，无法很好地捕捉数据中的模式。这通常意味着模型过于简单，不能有效地拟合训练数据的基本规律，导致训练误差和泛化误差都较高。欠拟合则表现为模型过于简单，不能有效拟合训练数据的模式，导致训练误差和泛化误差都较高。

## 评估方法

### 留出法 (Hold-out)

留出法是最简单的评估方法，它直接将数据集 $ D $ 划分为两个互斥的集合：训练集 $ S $ 和测试集 $ T $。训练集用于训练模型，测试集用于评估模型的性能。由于操作简单且易于实现，这种方法在实践中经常被使用。

- 数据集划分为：  
  $$
  D = S \cup T
  $$
  
- 且 $ S \cap T = \emptyset $ （训练集和测试集互斥）。

留出法的缺点在于，评估结果可能依赖于具体的划分方式，容易受到数据划分的影响。

### 交叉验证法 (Cross Validation)

交叉验证法将数据集 $ D $ 划分为 $ k $ 个大小相同的互斥子集，每次使用 $ k-1 $ 个子集进行训练，剩下的 1 个子集进行测试。这种方法能够更稳定地评估模型的性能，常用于模型选择和参数调优。

- 数据集划分为：  
  $$
  D = D_1 \cup D_2 \cup \dots \cup D_k
  $$
  
- 且 $ D_i \cap D_j = \emptyset $ （当 $ i \neq j $ 时）。

交叉验证的优点是能够较全面地使用数据，减少数据划分带来的不确定性。

#### K 折交叉验证 (K-fold Cross Validation)

K 折交叉验证是交叉验证的一种常见形式。数据集被划分为 $ k $ 个子集，每次使用 $ k-1 $ 个子集的并集作为训练集，余下的 1 个子集作为测试集。如此重复 $ k $ 次训练与测试，并取所有测试结果的平均值作为模型的性能指标。

- 训练集和测试集的划分：  
  $$
  \text{训练集} = \bigcup_{i \neq j} D_i, \quad \text{测试集} = D_j
  $$
  
- 结果为 $ k $ 次测试结果的平均值。

#### 留一法 (Leave One Out, LOO)

留一法是 K 折交叉验证的特例，当 $ k $ 等于数据集中样本数时，就是留一法。每次使用所有样本中的 $ n-1 $ 个样本作为训练集，剩下的 1 个样本作为测试集，重复 $ n $ 次，最终的评估结果是 $ n $ 个测试结果的平均值。留一法由于每次只留下一个样本进行测试，因此样本的影响较小，但计算复杂度较高。

- 每次的训练集和测试集：  
  $$
  \text{训练集} = D \setminus \{x_i\}, \quad \text{测试集} = \{x_i\}
  $$
  
- 总共进行 $ n $ 次训练与测试。

### 自助法 (Bootstrapping)

自助法是一种基于自主采样的技术。在每次采样时，从初始数据集 $ D $ 中随机选择一个样本，并将其拷贝到新数据集 $ D' $ 中。被采样的样本不会从原始数据集中移除，因此该样本在之后的采样过程中仍然有可能被再次选中。

这个过程重复执行 $ m $ 次，最终得到包含 $ m $ 个样本的数据集 $ D' $。在此过程中，某个样本在 $ m $ 次采样中始终未被采到的概率可以计算为：

$$
\lim_{m \to \infty} \left(1 - \frac{1}{m}\right)^m = \frac{1}{e} \approx 0.368
$$

因此，约有 $ 36.8\% $ 的样本不会被选中。我们可以将 $ D' $ 用作训练集，将 $ D \setminus D' $ 用作测试集。

- **优点**：自助法可以通过自主采样的方式从数据集中生成多个不同的训练集，特别适用于数据集较小或无法轻易划分的情况。
- **缺点**：由于采样过程中允许样本重复，自助法可能改变原始数据集的分布，从而引入偏差。

---

### 补充：分层采样与自助采样

#### 分层采样 (Stratified Sampling)

分层采样是一种改进的数据集划分方法，主要用于保证训练集和测试集中各类别样本的比例与原始数据集一致，避免类别不平衡对模型评估结果的影响。在交叉验证中，分层采样特别适用于分类问题，确保每个子集中的类别分布与原始数据集的类别分布相似。

- **作用**：在交叉验证或留出法中，通过分层采样，能尽可能减少因类别不平衡带来的误差，使得评估结果更加可靠，特别是在类别样本比例差异较大的情况下。

#### 自主采样 (Self-sampling)

自主采样是一种不放回的随机抽样方法。在自主采样中，每个样本被抽取一次且不会被重新放回原始数据集中。这种方法与自助法的最大区别在于，自主采样中样本只能被选中一次，不会重复。

- **作用**：自主采样可用于生成训练集和测试集，保证每个样本只参与一次训练或测试，适用于数据量较大且数据分布均衡的情况。





