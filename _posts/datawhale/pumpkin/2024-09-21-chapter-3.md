---
title: "🍉🎃 第 3 章: 线性模型"
description: 
author: yoyo
date: 2024-09-21 12:00:00 +0800
categories: [Artificial Intelligence, Datawhale, Pumpkin]
tags: [Machine Learning Theory]
math: true
lang: zh-CN
---

## 线性模型

给定一个包含 $$ d $$ 个属性的示例 $$ x = (x_1, x_2, ..., x_d) $$，我们试图学习一个权重向量 $$ \omega = (\omega_1, \omega_2, ..., \omega_d) $$ 和偏置项 $$ b $$，从而确定用于预测的线性函数。

线性模型的预测函数可以表示为：

$$
f(x) = \omega_1 x_1 + \omega_2 x_2 + \dots + \omega_d x_d + b
$$

一般情况下，可以用向量的形式简洁地表示为：

$$
f(x) = \omega^T x + b
$$

其中，$$ \omega^T x $$ 表示权重向量 $$ \omega $$ 和属性向量 $$ x $$ 的内积，$$ b $$ 为偏置项。

线性模型形式简单，易于建模，具有较强的可解释性（comprehensibility and understandability）。权重 $$ \omega $$ 直观地表示了各属性在预测中的重要性，因此也被称为权重（weight）。模型的每个权重可以解释该属性对预测结果的贡献。

## 线性回归 Linear regression

给定数据集 $$ D = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\} $$，即 $$ D = \{(x_i, y_i)\}_{i=1}^m $$，其中 $$ x_i $$ 为输入，$$ y_i $$ 为对应的输出值。线性回归的目标是通过数据集学习一个线性模型，用来预测新的数据点。

- 对存在“序”(order)关系的离散属性：
  - 通过连续化江其转化为连续值
  - 例如，属性“高度”具有“高”、“中”、“低”三个离散取值，但它们之间存在序关系，可以将其转化为连续值，如 {1.0, 0.5, 0.0}。
- 对不存在“序”关系的离散属性：
  - 若对无序属性进行连续化，会不恰当地引入序关系，可能导致误导性的处理（如在距离计算中）。通常，将 $$ k $$ 个无序属性值转化为 $$ k $$ 维向量（也称为“独热编码”或“one-hot encoding”）。
  - 例如，属性“类别”取值为“A”、“B”、“C”，可以将其编码为 (0, 0, 1)、(0, 1, 0)、(1, 0, 0)，而不会引入错误的顺序关系。
 
**序 Order**：序关系指属性值之间存在先后或大小关系。例如，“高”、“中”、“低”有明显的序关系，而“红”、“绿”、“蓝”则没有序关系。

线性回归试图学习如下形式的线性模型：

$$
f(x_i) = \omega^T x_i + b
$$

线性回归的目标是使模型 $$ f(x_i) $$ 对应的预测值尽可能接近真实值 $$ y_i $$，即：

$$
f(x_i) \approx y_i
$$

### 损失函数 cost function

#### 均分方差 Mean square error (MSE)

**均分方差**也称为**平方损失函数** (Square Loss)。这一损失函数对应了常用的欧几里得距离（简称“欧氏距离” Euclidean distance）。损失函数定义为：

$$
(\omega^*, b^*) = \underset{\omega, b}{\arg\min} \sum_{i=1}^{m} (f(x_i) - y_i)^2
$$

MSE 对大误差惩罚较大，适用于误差服从正态分布的场景。

#### 平均绝对误差 Mean Absolute Error (MAE)

计算预测值与真实值的绝对差值，公式如下：

$$
\text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |f(x_i) - y_i|
$$

MAE 对离群点敏感度较低，适用于含有噪声或离群点的数据集。

#### Huber 损失 Huber Loss

结合了 MSE 和 MAE 的优点，适用于既包含小误差又包含离群点的场景：

$$
L_{\delta}(a) =
\begin{cases} 
    \frac{1}{2}a^2 & \text{if } |a| \leq \delta \\
    \delta(|a| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

其中 $$ a = f(x_i) - y_i $$， $$ \delta $$ 是调节参数。

#### 对数损失 Logarithmic Loss (Log Loss)

用于分类任务，尤其是二分类问题。计算模型预测的概率与真实标签之间的差异：

$$
\text{Log Loss} = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(f(x_i)) + (1 - y_i) \log(1 - f(x_i)) \right]
$$

#### 交叉熵损失 Cross Entropy Loss

用于多分类问题，衡量模型预测的类别分布与真实分布之间的差异：

$$
\text{Cross Entropy} = -\sum_{i=1}^{m} \sum_{c=1}^{C} y_{ic} \log(f_{ic})
$$

其中 $$ C $$ 是类别数，$$ y_{ic} $$ 是样本 $$ i $$ 的真实类别，$$ f_{ic} $$ 是预测概率。


### 最小二乘法 least square method

最小二乘法是一种优化策略，目的是最小化**均方误差**。通过最小化损失函数，最小二乘法能够找到最优的参数 $$ \omega $$ 和 $$ b $$，从而使得模型的预测值与真实值之间的差异最小。这一优化过程通常被称为线性回归模型的**最小二乘参数估计** (parameter estimation)。

### 多元线性回归 Multiple linear regression

当样本由 $$ d > 1 $$ 个属性描述时，多元线性回归模型可以表示为：

$$
f(x_i) = \omega^T x_i + b
$$

通过最小化损失函数找到最优的 $$ \omega $$，多元线性回归的参数估计公式为：

$$
\hat{\omega} = \underset{\omega}{\arg\min} \ (y - X\hat{\omega})^T (y - X\hat{\omega})
$$

## 广义线性模型 Generalized Linear Model (GLM)

广义线性模型扩展了线性回归的框架，允许通过**联系函数** (Link Function) 来处理不同类型的输出变量。GLM 的模型形式为：

$$
y = g^{-1}(\omega^T x + b)
$$

其中，$$ g(·) $$ 是联系函数，通常是一个单调可微的函数，用来连接输入变量的线性组合与输出变量的期望值。联系函数是连续且充分光滑的，能够将线性模型应用于更广泛的问题，如分类、计数数据和比例数据。

### 参数估计

在广义线性模型中，参数 $$ \omega $$ 和 $$ b $$ 通常通过以下两种方法进行估计：

#### 加权最小二乘法 Weighted Least Squares (WLS)

加权最小二乘法是广义线性模型中常用的一种参数估计方法，特别适用于数据中方差不恒定的情况。与普通最小二乘法不同，加权最小二乘法通过为每个数据点赋予一个权重，以反映其重要性或不确定性。其目标是最小化加权平方误差，即：

$$
(\omega^*, b^*) = \underset{\omega, b}{\arg\min} \sum_{i=1}^{m} w_i (y_i - f(x_i))^2
$$

其中，$$ w_i $$ 是每个数据点的权重，反映了该点的方差。加权最小二乘法在处理异方差（方差随输入值变化）时非常有效。

#### 极大似然估计法 Maximum Likelihood Estimation (MLE)

极大似然估计法是一种广泛应用于统计模型中的参数估计方法，用于在给定观测数据的情况下，找到最有可能产生这些数据的模型参数。对于广义线性模型，极大似然估计通过最大化似然函数 $$ L(\omega, b) $$ 来估计模型参数：

$$
(\omega^*, b^*) = \underset{\omega, b}{\arg\max} \ L(\omega, b)
$$

似然函数 $$ L $$ 是基于数据点的联合概率分布构建的。通过最大化似然函数，可以找到最能解释观测数据的参数组合。极大似然法是许多统计模型中的标准方法，尤其在处理带有随机误差的数据时表现优越。

- **应用场景**：在广义线性模型中，MLE 特别适用于当输出变量来自于某种概率分布（如二项分布或泊松分布）时。它能够为分类问题（如逻辑回归）或计数数据建模提供有效的参数估计。

### 对数线性回归 log-linear regression

对数线性回归时光一线性模型在 $$ g(·) = ln(·）$$ 时的特例。在对数线性回归中，模型的目标是让 $$ e^{\omega^T x_i + b} $$ 逼近 $$ y $$。虽然形式上仍然是线性回归，但通过对预测值 $$ y $$ 进行指数运算，模型实际上是在求输入空间到输出空间的**非线性**函数映射：

$$
f(x_i) = e^{\omega^T x_i + b}
$$

[image]: log-linear-regression.jpeg

这种模型常用于处理对数分布的目标变量，尤其是在处理某些特定类型的计数数据或概率时非常有效。形式上属于广义线性模型的一部分，但由于输出值是非线性函数的结果，因此实质上是一种非线性回归。

### 对数几率回归 (逻辑回归 Logistic regression)

逻辑回归是一种分类模型，它将实值 $$ z = \omega^T x + b $$ 转换为 0/1 的分类结果。

#### 单位阶跃函数 (Unit Step Function)

单位阶跃函数将实值 $$ z $$ 转换为离散的 0 或 1 值：

$$
y =
\begin{cases}
0 & \text{if } z < 0 \\
0.5 & \text{if } z = 0 \\
1 & \text{if } z > 0
\end{cases}
$$

**缺点**：单位阶跃函数不连续，无法用于模型的优化（如梯度下降），因为它不可导。
  
#### 对数几率函数 Logistic function

对数几率函数是单调可微的，能够在一定程度上近似单位阶跃函数。它将实值 $$ z $$ 映射为 0 到 1 之间的概率值：

$$
y = \frac{1}{1 + e^{-z}}
$$

这是逻辑回归中常用的**Sigmoid函数**，其输出值可以解释为样本属于正类的概率。

[image]: logistic-regression (这一行不用改）

Sigmoid函数还可以表示为：

$$
\ln \frac{y}{1 - y} = \omega^T x + b
$$

其中，$$ y $$ 表示样本为正例的概率，$$ 1 - y $$ 表示样本为反例的概率。两者的比值 $$ \frac{y}{1 - y} $$ 称为**几率** (odds)，反映了样本作为正例的相对可能性。对几率取对数得到 **对数几率** (log odds)：

$$
\ln \frac{y}{1 - y}
$$

- **解释**：当 $$ y $$ 被解释为样本作为正例的概率时，$$ \frac{y}{1 - y} $$ 就是正例和反例的概率比值。对几率取对数后，得到的结果与输入特征的线性组合 $$ \omega^T x + b $$ 线性相关。

### 逻辑回归的特点

- **概率建模**：逻辑回归不仅仅给出分类结果，还能提供类别的概率估计。
- **可导性**：对数几率函数是一个任意阶可导的凸函数，适合使用梯度下降等优化方法进行训练。
- **无需假设数据分布**：逻辑回归无需像朴素贝叶斯那样对数据的分布作假设，因此更加灵活，适用于各种分类问题。

