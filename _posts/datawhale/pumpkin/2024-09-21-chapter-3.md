---
title: "🍉🎃 第 3 章: 线性模型"
description: 
author: yoyo
date: 2024-09-21 12:00:00 +0800
categories: [Artificial Intelligence, Datawhale, Pumpkin]
tags: [Machine Learning Theory]
math: true
lang: zh-CN
---

## 线性模型

给定一个包含 \( d \) 个属性的示例 \( x = (x_1, x_2, ..., x_d) \)，我们试图学习一个权重向量 \( \omega = (\omega_1, \omega_2, ..., \omega_d) \) 和偏置项 \( b \)，从而确定用于预测的线性函数。

线性模型的预测函数可以表示为：

$$
f(x) = \omega_1 x_1 + \omega_2 x_2 + \dots + \omega_d x_d + b
$$

一般情况下，可以用向量的形式简洁地表示为：

$$
f(x) = \omega^T x + b
$$

其中，\( \omega^T x \) 表示权重向量 \( \omega \) 和属性向量 \( x \) 的内积，\( b \) 为偏置项。

线性模型形式简单，易于建模，具有较强的可解释性（comprehensibility and understandability）。权重 \( \omega \) 直观地表示了各属性在预测中的重要性，因此也被称为权重（weight）。模型的每个权重可以解释该属性对预测结果的贡献。

## 线性回归 Linear regression

给定数据集 \( D = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\} \)，即 \( D = \{(x_i, y_i)\}_{i=1}^m \)，其中 \( x_i \) 为输入，\( y_i \) 为对应的输出值。线性回归的目标是通过数据集学习一个线性模型，用来预测新的数据点。

- 对存在“序”(order)关系的离散属性：
  - 通过连续化江其转化为连续值
  - 例如，属性“高度”具有“高”、“中”、“低”三个离散取值，但它们之间存在序关系，可以将其转化为连续值，如 {1.0, 0.5, 0.0}。
- 对不存在“序”关系的离散属性：
  - 若对无序属性进行连续化，会不恰当地引入序关系，可能导致误导性的处理（如在距离计算中）。通常，将 \( k \) 个无序属性值转化为 \( k \) 维向量（也称为“独热编码”或“one-hot encoding”）。
  - 例如，属性“类别”取值为“A”、“B”、“C”，可以将其编码为 (0, 0, 1)、(0, 1, 0)、(1, 0, 0)，而不会引入错误的顺序关系。
 
**序 Order**：序关系指属性值之间存在先后或大小关系。例如，“高”、“中”、“低”有明显的序关系，而“红”、“绿”、“蓝”则没有序关系。

线性回归试图学习如下形式的线性模型：

$$
f(x_i) = \omega^T x_i + b
$$

其中，\( \omega \) 为权重向量，\( x_i \) 为输入属性向量，\( b \) 为偏置项。线性回归的目标是使模型 \( f(x_i) \) 对应的预测值尽可能接近真实值 \( y_i \)，即：

$$
f(x_i) \approx y_i
$$

### 损失函数 cost function

#### 均分方差 Mean square error (MSE)

**均分方差**也称为**平方损失函数** (Square Loss)。这一损失函数对应了常用的欧几里得距离（简称“欧氏距离” Euclidean distance）。损失函数定义为：

$$
(\omega^*, b^*) = \underset{\omega, b}{\arg\min} \sum_{i=1}^{m} (f(x_i) - y_i)^2
$$

MSE 对大误差惩罚较大，适用于误差服从正态分布的场景。

#### 平均绝对误差 Mean Absolute Error (MAE)

计算预测值与真实值的绝对差值，公式如下：

$$
\text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |f(x_i) - y_i|
$$

MAE 对离群点敏感度较低，适用于含有噪声或离群点的数据集。

#### Huber 损失 Huber Loss

结合了 MSE 和 MAE 的优点，适用于既包含小误差又包含离群点的场景：

$$
L_{\delta}(a) =
\begin{cases} 
    \frac{1}{2}a^2 & \text{if } |a| \leq \delta \\
    \delta(|a| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

其中 \( a = f(x_i) - y_i \)， \( \delta \) 是调节参数。

#### 对数损失 Logarithmic Loss (Log Loss)

用于分类任务，尤其是二分类问题。计算模型预测的概率与真实标签之间的差异：

$$
\text{Log Loss} = -\frac{1}{m} \sum_{i=1}^{m} \left[ y_i \log(f(x_i)) + (1 - y_i) \log(1 - f(x_i)) \right]
$$

#### 交叉熵损失 Cross Entropy Loss

用于多分类问题，衡量模型预测的类别分布与真实分布之间的差异：

$$
\text{Cross Entropy} = -\sum_{i=1}^{m} \sum_{c=1}^{C} y_{ic} \log(f_{ic})
$$

其中 \( C \) 是类别数，\( y_{ic} \) 是样本 \( i \) 的真实类别，\( f_{ic} \) 是预测概率。


### 最小二乘法 least square method

最小二乘法是一种优化策略，目的是最小化**均方误差**。通过最小化损失函数，最小二乘法能够找到最优的参数 \( \omega \) 和 \( b \)，从而使得模型的预测值与真实值之间的差异最小。这一优化过程通常被称为线性回归模型的**最小二乘参数估计** (parameter estimation)。

### 多元线性回归 Multiple linear regression

当样本由d > 1个属性描述时

$$
f(x_i) = \omega ^ T x_i + b

所以 omega【omega上面有一个^】 = argmin 【下面有一个omega带着^】(y - X_omega带个^）^T (y - X_omega带个^)

### 对数线性回归 log-linear regression

让e^(omega^T + b) 逼近y。形式上仍是线性回归，实质上是在求去输入空间到输出空间的非线性函数映射。

[image]:

